{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import re\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Data/tv_shows.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "info=data[['Title','Year']]\n",
    "scrap_names=[x.replace(' ','%20') for x in data.Title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Breaking%20Bad',\n",
       " 'Stranger%20Things',\n",
       " 'Money%20Heist',\n",
       " 'Sherlock',\n",
       " 'Better%20Call%20Saul',\n",
       " 'The%20Office',\n",
       " 'Black%20Mirror',\n",
       " 'Supernatural',\n",
       " 'Peaky%20Blinders',\n",
       " 'Avatar:%20The%20Last%20Airbender']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrap_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectMainSpider:\n",
    "    \"\"\"\n",
    "    This is the constructor class to which you can pass a bunch of parameters. \n",
    "    These parameters are stored to the class instance variables so that the\n",
    "    class functions can access them later.\n",
    "    \n",
    "    url_pattern: the regex pattern of the web urls to scape\n",
    "    pages_to_scrape: how many pages to scrape\n",
    "    sleep_interval: the time interval in seconds to delay between requests. If <0, requests will not be delayed.\n",
    "    content_parser: a function reference that will extract the intended info from the scraped content.\n",
    "    \"\"\"\n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=True, get_agent=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.get_agent=get_agent\n",
    "        self.content_parser = None\n",
    "        self.fails=[]\n",
    "        self.fails2=[]\n",
    "        self.links=[]\n",
    "        self.titles=[]\n",
    "        self.country=[]\n",
    "        self.language=[]\n",
    "        self.runtime=[]\n",
    "        self.genre=[]\n",
    "        self.creator=[]\n",
    "        self.episode=[]\n",
    "        self.soups=[]\n",
    "\n",
    "\n",
    "    async def fetch_headers(self,url,headers):\n",
    "        async with aiohttp.ClientSession() as s, s.get(url,headers=headers) as res:\n",
    "            ret=await res.read()\n",
    "            status=res.status\n",
    "            return status,ret\n",
    "    \"\"\"\n",
    "    Scrape the content of a single url.\n",
    "    \"\"\"\n",
    "    \n",
    "    async def scrape_url(self,url,comb):\n",
    "        agent=self.get_agent()\n",
    "        headers = {\n",
    "            'user-agent': agent,\n",
    "            'authority': 'www.imdb.com',\n",
    "            'accept': 'application/json, text/plain, */*',\n",
    "            'accept-encoding': 'gzip, deflate, br',\n",
    "            'accept-language': 'en-US,en;q=0.9,es;q=0.8,fr;q=0.7,gl;q=0.6',\n",
    "            'referer':'https://www.google.com'\n",
    "        } \n",
    "        try:\n",
    "            status,content= await self.fetch_headers(url,headers)\n",
    "            if status<300:\n",
    "                self.soups.append(content)\n",
    "                result = self.content_parser(content,comb)\n",
    "            elif status in range(400,500):\n",
    "                print('Error {} in \"{}\", request failed because the resource either does not exist or is forbidden.'\n",
    "                      .format(status,url))\n",
    "                try:\n",
    "                    self.fails.append(comb.Title)\n",
    "                except:\n",
    "                    self.fails2.append(url)\n",
    "            else:\n",
    "                print('Error {}, in \"{}\", request failed because the response server encountered an error.'\n",
    "                      .format(status,url))\n",
    "                try:\n",
    "                    self.fails.append(comb.Title)\n",
    "                except:\n",
    "                    self.fails2.append(url)\n",
    "        except aiohttp.ServerTimeoutError:\n",
    "            print('Error= Timeout.'.format(url))\n",
    "            try:\n",
    "                self.fails.append(comb.Title)\n",
    "            except:\n",
    "                self.fails2.append(url)\n",
    "        except aiohttp.TooManyRedirects:\n",
    "            print('Error= TooManyRedirects, in {}.'.format(url))\n",
    "            try:\n",
    "                self.fails.append(comb.Title)\n",
    "            except:\n",
    "                self.fails2.append(url)\n",
    "        except aiohttp.ClientSSLError:\n",
    "            print('Error= SSLError, in {}.'.format(url))\n",
    "            try:\n",
    "                self.fails.append(comb.Title)\n",
    "            except:\n",
    "                self.fails2.append(url)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print('Error= {}, in {}.'.format(e,url))\n",
    "            try:\n",
    "                self.fails.append(comb.Title)\n",
    "            except:\n",
    "                self.fails2.append(url)\n",
    "    \n",
    "    def random_interval(self):\n",
    "        delays = [1, 2, 3, 4, 5, 7]\n",
    "        perm = np.random.RandomState()\n",
    "        #return random x in [0.5,n+0.5]\n",
    "        return(perm.permutation(range(np.random.choice(delays)*100))[0]+5)/100\n",
    "    \"\"\"\n",
    "    After the class is instantiated, call this function to start the scraping jobs.\n",
    "    This function uses a FOR loop to call `scrape_url()` for each url to scrape.\n",
    "    \"\"\"\n",
    "    async def kickstart(self,parser,main=True):\n",
    "        self.content_parser=parser\n",
    "        self.fails=[]\n",
    "        self.fails2=[]\n",
    "        #Get links from search\n",
    "        if main:\n",
    "            #send requests in n groups of n where n=sqrt(total requests)\n",
    "            for j in range(int(np.sqrt(self.pages_to_scrape))):\n",
    "                await asyncio.wait([self.scrape_url(self.url_pattern.format(scrap_names[i]),info.loc[i]) for i in range(self.pages_to_scrape)\n",
    "                                   if i%(int(np.sqrt(self.pages_to_scrape)))==j])\n",
    "                if self.sleep_interval:\n",
    "                    await asyncio.sleep(self.random_interval())\n",
    "                #Progress indicator\n",
    "                print('Group {} finished from {}'.format(j+1,int(np.sqrt(self.pages_to_scrape))))\n",
    "        #Get info from each tv show\n",
    "        else:\n",
    "            #send requests in n groups of n where n=sqrt(total requests)\n",
    "            for j in range(int(np.sqrt(len(self.links)))):\n",
    "                await asyncio.wait([self.scrape_url(i[0],i[1]) for x,i in enumerate(self.links) \n",
    "                                    if x%(int(np.sqrt(len(self.links))))==j])\n",
    "                if self.sleep_interval:\n",
    "                    await asyncio.sleep(self.random_interval())\n",
    "                    #Progress indicator\n",
    "                print('Group {} finished from {}'.format(j+1,int(np.sqrt(len(self.links)))))\n",
    "            #Save data in DF\n",
    "            data=zip(self.titles,self.country,self.language,self.runtime,self.genre,self.creator,self.episode)\n",
    "            self.df=pd.DataFrame(data,columns=['Title','Country','Language','Runtime','Genre','Creator','Episodes'])\n",
    "        print('Done')\n",
    "            \n",
    "    def async_kickstart(self,parser,main=True):\n",
    "        try:\n",
    "            loop=asyncio.get_event_loop()\n",
    "            loop.run_until_complete(self.kickstart(parser,main))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "                \n",
    "    def links_parser(self,content,comb):\n",
    "        soup=BeautifulSoup(content)\n",
    "        try:\n",
    "            temp_links=[]\n",
    "            for x in soup.find_all('td',{'class':'result_text'}):\n",
    "                temp_links.append([x.a['href'], x.text])\n",
    "            for x,y in temp_links:\n",
    "                if re.findall(str(comb.Year),y):\n",
    "                    link=x\n",
    "                    break\n",
    "            self.links.append(['https://www.imdb.com'+x,comb.Title])\n",
    "        except:\n",
    "            self.fails.append(comb.Title)\n",
    "    def data_parser(self,content,name):\n",
    "        soup=BeautifulSoup(content)\n",
    "        try:\n",
    "            details=soup.select('#titleDetails')[0].text\n",
    "            details=re.sub('\\xa0|\\n',' ',details)\n",
    "        except:\n",
    "            details='none'\n",
    "        all_dets=re.findall('Country: *((?:\\S*(?: *\\| *)?)*)|Language: *((?:\\S*(?: *\\| *)?)*)|Runtime: *(\\S*)',details)\n",
    "        try:\n",
    "            story_line=soup.select('#titleStoryLine')[0].text\n",
    "            story_line=re.sub('\\xa0|\\n',' ',story_line)\n",
    "        except:\n",
    "            story_line='none'\n",
    "        try:\n",
    "            country=all_dets[0][0]\n",
    "            country=re.sub(' ','',country)\n",
    "            country=re.sub('\\|',',',country)\n",
    "        except:\n",
    "            country=np.NaN\n",
    "        country\n",
    "        try:\n",
    "            language=all_dets[1][1]\n",
    "            language=re.sub(' ','',language)\n",
    "            language=re.sub('\\|',',',language)\n",
    "        except:\n",
    "            language=np.NaN\n",
    "        try:\n",
    "            runtime=all_dets[2][2]\n",
    "        except:\n",
    "            runtime=np.NaN\n",
    "        try:\n",
    "            genre=re.findall('Genres: *((?:\\S*(?: \\| *)?)*)',story_line)[0]\n",
    "            genre=re.sub(' ','',genre)\n",
    "            genre=re.sub('\\|',',',genre)\n",
    "        except:\n",
    "            genre=np.NaN\n",
    "        try:\n",
    "            creators=soup.find('div',{'class':'credit_summary_item'})\n",
    "            creator=','.join([x.text for x in creators.find_all('a')])\n",
    "        except:\n",
    "            creator=np.NaN\n",
    "        try:\n",
    "            episode=soup.select('.bp_content .bp_sub_heading')[0].text\n",
    "            episode=re.findall('\\d+',episode)[0]\n",
    "        except:\n",
    "            episode=np.NaN\n",
    "        self.titles.append(name)\n",
    "        self.country.append(country)\n",
    "        self.language.append(language)\n",
    "        self.runtime.append(runtime)\n",
    "        self.genre.append(genre)\n",
    "        self.creator.append(creator)\n",
    "        self.episode.append(episode)\n",
    "\n",
    "\n",
    "def random_agent():\n",
    "    with open('agents.txt',encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        perm = np.random.RandomState()\n",
    "        index = perm.permutation(len(lines) - 1)[0]\n",
    "        agent = lines[int(index)].strip()\n",
    "        return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error= , in https://www.imdb.com/find?q=En%20La%20Boca%20Del%20Lobo&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Group 1 finished from 74\n",
      "Group 2 finished from 74\n",
      "Error= [WinError 10053] Se ha anulado una conexión establecida por el software en su equipo host, in https://www.imdb.com/find?q=Golden%20Time&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Group 3 finished from 74\n",
      "Group 4 finished from 74\n",
      "Error= , in https://www.imdb.com/find?q=Miami%20Ink&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Nightmare%20in%20Suburbia&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=The%20Mindy%20Project&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=On%20Children&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Jake%20and%20the%20Never%20Land%20Pirates&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Hunter%20Street&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Longmire&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Group 5 finished from 74\n",
      "Group 6 finished from 74\n",
      "Group 7 finished from 74\n",
      "Group 8 finished from 74\n",
      "Group 9 finished from 74\n",
      "Group 10 finished from 74\n",
      "Group 11 finished from 74\n",
      "Group 12 finished from 74\n",
      "Group 13 finished from 74\n",
      "Error= , in https://www.imdb.com/find?q=Sea%20Patrol&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=My%20Little%20Pony:%20Equestria%20Girls&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Group 14 finished from 74\n",
      "Group 15 finished from 74\n",
      "Group 16 finished from 74\n",
      "Group 17 finished from 74\n",
      "Error= [WinError 10053] Se ha anulado una conexión establecida por el software en su equipo host, in https://www.imdb.com/find?q=Serial%20Killer%20with%20Piers%20Morgan&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= [WinError 10053] Se ha anulado una conexión establecida por el software en su equipo host, in https://www.imdb.com/find?q=Can't%20Cope,%20Won't%20Cope&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= [WinError 10053] Se ha anulado una conexión establecida por el software en su equipo host, in https://www.imdb.com/find?q=The%20Could’ve-Gone-All-the-Way%20Committee&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= [WinError 10053] Se ha anulado una conexión establecida por el software en su equipo host, in https://www.imdb.com/find?q=Betas&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Group 18 finished from 74\n",
      "Group 19 finished from 74\n",
      "Group 20 finished from 74\n",
      "Group 21 finished from 74\n",
      "Group 22 finished from 74\n",
      "Group 23 finished from 74\n",
      "Group 24 finished from 74\n",
      "Group 25 finished from 74\n",
      "Group 26 finished from 74\n",
      "Group 27 finished from 74\n",
      "Group 28 finished from 74\n",
      "Group 29 finished from 74\n",
      "Group 30 finished from 74\n",
      "Group 31 finished from 74\n",
      "Error= , in https://www.imdb.com/find?q=Casper%20and%20Friends&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Grand%20Designs&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=The%20X-Files&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Chopped%20Junior&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Year%20Million&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Ultimate%20Survival%20WWII&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= [WinError 10053] Se ha anulado una conexión establecida por el software en su equipo host, in https://www.imdb.com/find?q=Hyori's%20Bed%20and%20Breakfast&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Baby%20Talk&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Charlie%20Jade&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=44%20Cats&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= [WinError 10053] Se ha anulado una conexión establecida por el software en su equipo host, in https://www.imdb.com/find?q=Monzón:%20A%20Knockout%20Blow&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Group 32 finished from 74\n",
      "Group 33 finished from 74\n",
      "Group 34 finished from 74\n",
      "Group 35 finished from 74\n",
      "Group 36 finished from 74\n",
      "Group 37 finished from 74\n",
      "Group 38 finished from 74\n",
      "Group 39 finished from 74\n",
      "Group 40 finished from 74\n",
      "Group 41 finished from 74\n",
      "Group 42 finished from 74\n",
      "Error= [WinError 10053] Se ha anulado una conexión establecida por el software en su equipo host, in https://www.imdb.com/find?q=Carnivàle&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= [WinError 10053] Se ha anulado una conexión establecida por el software en su equipo host, in https://www.imdb.com/find?q=The%20Collection&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Game%20Winning%20Hit&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Group 43 finished from 74\n",
      "Group 44 finished from 74\n",
      "Group 45 finished from 74\n",
      "Group 46 finished from 74\n",
      "Group 47 finished from 74\n",
      "Group 48 finished from 74\n",
      "Group 49 finished from 74\n",
      "Error= , in https://www.imdb.com/find?q=I%20Love%20Toy%20Trains%20-%20I%20Love%20Big%20Trains&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Dead%20to%20Me&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Call%20My%20Agent&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Nailed%20It!%20Mexico&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Aesthetica%20of%20a%20Rogue%20Hero&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Naruto%20Shippūden&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Group 50 finished from 74\n",
      "Group 51 finished from 74\n",
      "Group 52 finished from 74\n",
      "Error= , in https://www.imdb.com/find?q=Titanic&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Seeking%20My%20Own%20Fortune&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=The%20Mysterious%20Play&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=TURN:%20Washington's%20Spies&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Nailed%20It!%20Spain&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Leo%20the%20Truck&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Nursery%20Rhymes%20For%20Babies%20and%20Toddlers&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Blood%20Blockade%20Battlefront&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Sacred%20Games&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Group 53 finished from 74\n",
      "Group 54 finished from 74\n",
      "Error= [WinError 10053] Se ha anulado una conexión establecida por el software en su equipo host, in https://www.imdb.com/find?q=Blue%20City&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Group 55 finished from 74\n",
      "Group 56 finished from 74\n",
      "Group 57 finished from 74\n",
      "Group 58 finished from 74\n",
      "Error= , in https://www.imdb.com/find?q=Cape%20Town&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=My%20First%20First%20Love&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=God%20Eater&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Group 59 finished from 74\n",
      "Group 60 finished from 74\n",
      "Group 61 finished from 74\n",
      "Group 62 finished from 74\n",
      "Group 63 finished from 74\n",
      "Group 64 finished from 74\n",
      "Group 65 finished from 74\n",
      "Group 66 finished from 74\n",
      "Group 67 finished from 74\n",
      "Group 68 finished from 74\n",
      "Error= , in https://www.imdb.com/find?q=Colony&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Will%20&%20Grace&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Jamie%20and%20Jimmy's%20Food%20Fight%20Club&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= [WinError 10053] Se ha anulado una conexión establecida por el software en su equipo host, in https://www.imdb.com/find?q=Agent&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Group 69 finished from 74\n",
      "Group 70 finished from 74\n",
      "Group 71 finished from 74\n",
      "Group 72 finished from 74\n",
      "Group 73 finished from 74\n",
      "Error= , in https://www.imdb.com/find?q=Good%20Morning%20America&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Pingu&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Error= , in https://www.imdb.com/find?q=Escape%20to%20the%20Country&s=tt&ttype=tv&ref_=fn_tv.\n",
      "Group 74 finished from 74\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "URL_PATTERN = 'https://www.imdb.com/find?q={}&s=tt&ttype=tv&ref_=fn_tv'\n",
    "PAGES_TO_SCRAPE = len(scrap_names)\n",
    "my_main_spider = ProjectMainSpider(URL_PATTERN, PAGES_TO_SCRAPE,get_agent=random_agent,sleep_interval=True)\n",
    "#get links\n",
    "my_main_spider.async_kickstart(my_main_spider.links_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "info=data[data.Title.isin(my_main_spider.fails)][['Title','Year']]\n",
    "info.reset_index(inplace=True,drop=True)\n",
    "scrap_names=[x.replace(' ','%20') for x in data[data.Title.isin(my_main_spider.fails)].Title]\n",
    "my_main_spider.pages_to_scrape = len(scrap_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_main_spider.fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5249"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_main_spider.links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 finished from 19\n",
      "Group 2 finished from 19\n",
      "Group 3 finished from 19\n",
      "Group 4 finished from 19\n",
      "Group 5 finished from 19\n",
      "Group 6 finished from 19\n",
      "Group 7 finished from 19\n",
      "Group 8 finished from 19\n",
      "Group 9 finished from 19\n",
      "Group 10 finished from 19\n",
      "Group 11 finished from 19\n",
      "Group 12 finished from 19\n",
      "Group 13 finished from 19\n",
      "Group 14 finished from 19\n",
      "Group 15 finished from 19\n",
      "Group 16 finished from 19\n"
     ]
    }
   ],
   "source": [
    "my_main_spider.async_kickstart(my_main_spider.links_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 finished from 72\n",
      "Group 2 finished from 72\n",
      "Group 3 finished from 72\n",
      "Group 4 finished from 72\n",
      "Group 5 finished from 72\n",
      "Group 6 finished from 72\n",
      "Group 7 finished from 72\n",
      "Group 8 finished from 72\n",
      "Group 9 finished from 72\n",
      "Group 10 finished from 72\n",
      "Group 11 finished from 72\n",
      "Group 12 finished from 72\n",
      "Group 13 finished from 72\n",
      "Group 14 finished from 72\n",
      "Group 15 finished from 72\n",
      "Group 16 finished from 72\n",
      "Group 17 finished from 72\n",
      "Group 18 finished from 72\n",
      "Group 19 finished from 72\n",
      "Group 20 finished from 72\n",
      "Group 21 finished from 72\n",
      "Group 22 finished from 72\n",
      "Group 23 finished from 72\n",
      "Group 24 finished from 72\n",
      "Group 25 finished from 72\n",
      "Group 26 finished from 72\n",
      "Group 27 finished from 72\n",
      "Group 28 finished from 72\n",
      "Group 29 finished from 72\n",
      "Group 30 finished from 72\n",
      "Group 31 finished from 72\n",
      "Group 32 finished from 72\n",
      "Group 33 finished from 72\n",
      "Group 34 finished from 72\n",
      "Group 35 finished from 72\n",
      "Group 36 finished from 72\n",
      "Group 37 finished from 72\n",
      "Group 38 finished from 72\n",
      "Group 39 finished from 72\n",
      "Group 40 finished from 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished coro=<ProjectMainSpider.scrape_url() done, defined at <ipython-input-6-8afc616ac529>:40> exception=TimeoutError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-8afc616ac529>\", line 51, in scrape_url\n",
      "    status,content= await self.fetch_headers(url,headers)\n",
      "  File \"<ipython-input-6-8afc616ac529>\", line 33, in fetch_headers\n",
      "    ret=await res.read()\n",
      "  File \"C:\\Users\\Usuario_Asignado\\Anaconda3\\envs\\ironhack_conda\\lib\\site-packages\\aiohttp\\client_reqrep.py\", line 973, in read\n",
      "    self._body = await self.content.read()\n",
      "  File \"C:\\Users\\Usuario_Asignado\\Anaconda3\\envs\\ironhack_conda\\lib\\site-packages\\aiohttp\\streams.py\", line 358, in read\n",
      "    block = await self.readany()\n",
      "  File \"C:\\Users\\Usuario_Asignado\\Anaconda3\\envs\\ironhack_conda\\lib\\site-packages\\aiohttp\\streams.py\", line 380, in readany\n",
      "    await self._wait('readany')\n",
      "  File \"C:\\Users\\Usuario_Asignado\\Anaconda3\\envs\\ironhack_conda\\lib\\site-packages\\aiohttp\\streams.py\", line 296, in _wait\n",
      "    await waiter\n",
      "  File \"C:\\Users\\Usuario_Asignado\\Anaconda3\\envs\\ironhack_conda\\lib\\site-packages\\aiohttp\\helpers.py\", line 596, in __exit__\n",
      "    raise asyncio.TimeoutError from None\n",
      "concurrent.futures._base.TimeoutError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 41 finished from 72\n",
      "Group 42 finished from 72\n",
      "Group 43 finished from 72\n",
      "Group 44 finished from 72\n",
      "Error= , in https://www.imdb.com/title/tt7985540/?ref_=fn_tv_tt_1.\n",
      "Error= , in https://www.imdb.com/title/tt2226342/?ref_=fn_tv_tt_1.\n",
      "Group 45 finished from 72\n",
      "Group 46 finished from 72\n",
      "Group 47 finished from 72\n",
      "Group 48 finished from 72\n",
      "Group 49 finished from 72\n",
      "Group 50 finished from 72\n",
      "Group 51 finished from 72\n",
      "Group 52 finished from 72\n",
      "Group 53 finished from 72\n",
      "Group 54 finished from 72\n",
      "Group 55 finished from 72\n",
      "Group 56 finished from 72\n",
      "Group 57 finished from 72\n",
      "Group 58 finished from 72\n",
      "Group 59 finished from 72\n",
      "Group 60 finished from 72\n",
      "Group 61 finished from 72\n",
      "Group 62 finished from 72\n",
      "Group 63 finished from 72\n",
      "Group 64 finished from 72\n",
      "Group 65 finished from 72\n",
      "Group 66 finished from 72\n",
      "Group 67 finished from 72\n",
      "Group 68 finished from 72\n",
      "Group 69 finished from 72\n",
      "Group 70 finished from 72\n",
      "Group 71 finished from 72\n",
      "Group 72 finished from 72\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#get data\n",
    "my_main_spider.async_kickstart(my_main_spider.data_parser,main=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_main_spider.links=[x for x in my_main_spider.links if x[0] in my_main_spider.fails2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 finished from 1\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "my_main_spider.async_kickstart(my_main_spider.data_parser,main=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5248 entries, 0 to 5247\n",
      "Data columns (total 7 columns):\n",
      "Title       5248 non-null object\n",
      "Country     5184 non-null object\n",
      "Language    5070 non-null object\n",
      "Runtime     3292 non-null object\n",
      "Genre       5118 non-null object\n",
      "Creator     4994 non-null object\n",
      "Episodes    4920 non-null object\n",
      "dtypes: object(7)\n",
      "memory usage: 287.1+ KB\n"
     ]
    }
   ],
   "source": [
    "my_main_spider.df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_main_spider.df.to_csv('Data/extra_tv_shows.csv',index=False,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
